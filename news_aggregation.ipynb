{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-pujara/bubble.ai/blob/main/news_aggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "eGZpHY-S2Ib9",
        "outputId": "df6825e3-4137-4da0-d5a9-ecdc2970166f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b413f1c0728>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnewsapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNewsApiClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'newsapi'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import time\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the News API client\n",
        "newsapi = NewsApiClient(api_key='74d7df82b0314dad9eb78ddd579748e3')\n",
        "\n",
        "# Function to extract necessary information from an article\n",
        "def extract_article_info(article):\n",
        "    return {\n",
        "        'source': article['source']['name'],\n",
        "        'author': article.get('author'),\n",
        "        'description': article.get('description'),\n",
        "        'title': article.get('title'),\n",
        "        'url': article.get('url'),\n",
        "        'urlToImage': article.get('urlToImage'),\n",
        "        'publishedAt': article.get('publishedAt'),\n",
        "        'content': article.get('content')\n",
        "    }\n",
        "\n",
        "# Function to fetch the latest articles for a given topic\n",
        "def fetch_latest_news(topic):\n",
        "    response = newsapi.get_everything(q=topic,\n",
        "                                      language='en',\n",
        "                                      sort_by='publishedAt',\n",
        "                                      page=1)\n",
        "    articles = response['articles']\n",
        "    return [extract_article_info(article) for article in articles]\n",
        "\n",
        "# List of topics to fetch news for\n",
        "topics = ['technology', 'finance', 'health', 'sports']  # Example topics\n",
        "\n",
        "iteration_count = 0\n",
        "max_iterations = 10  # Set a maximum number of iterations if needed\n",
        "\n",
        "while iteration_count < max_iterations:\n",
        "    all_articles_data = []\n",
        "    for topic in topics:\n",
        "        latest_articles = fetch_latest_news(topic)\n",
        "        all_articles_data.extend(latest_articles)\n",
        "\n",
        "    # Convert the articles data to a DataFrame\n",
        "    df_articles = pd.DataFrame(all_articles_data)\n",
        "\n",
        "    # Timestamp for the filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f'news_articles_{timestamp}.csv'\n",
        "\n",
        "    # Save to CSV\n",
        "    df_articles.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"Saved articles to {filename}\")\n",
        "\n",
        "    # Increment the iteration count and wait before the next iteration\n",
        "    iteration_count += 1\n",
        "    time.sleep(60)  # Wait for 1 hour (3600 seconds)\n",
        "\n",
        "print(\"Finished fetching articles.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3PsFMq92Mzj"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "with open('news_articles_20231206_063802.csv', 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncqt1x4O2qEc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('news_articles_20231206_063802.csv')\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in stop_words:\n",
        "            result.append(token)\n",
        "    return result\n",
        "\n",
        "# Apply preprocessing to the description column\n",
        "processed_docs = df['description'].map(preprocess)\n",
        "\n",
        "# Create a dictionary and corpus needed for LDA model\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=dictionary,\n",
        "                                       num_topics=10,  # You can choose the number of topics\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)\n",
        "\n",
        "# Function to get the dominant topic for each description\n",
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=processed_docs):\n",
        "    topic_list = []\n",
        "    for row in range(len(df)):\n",
        "        topic, _ = max(ldamodel[corpus[row]], key=lambda x: x[1])\n",
        "        topic_list.append(topic)\n",
        "    return topic_list\n",
        "\n",
        "# Get the dominant topic for each description\n",
        "df['Topic'] = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=processed_docs)\n",
        "\n",
        "# Save the enhanced DataFrame to a new CSV file\n",
        "df.to_csv('news_articles_with_topics.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C1iQJGTEFoU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iOXm8082pE7TMn7JalpAfVZDMEi-LXya",
      "authorship_tag": "ABX9TyM+OnyWoH0992KCJuDlyUKn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}